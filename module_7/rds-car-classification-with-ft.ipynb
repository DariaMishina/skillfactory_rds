{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Классификация изображений\n\n### Основная идея этого решения: взять предобученую на ImageNet сеть и дообучить под нашу задачу. \n\nВ этом ноутбуке описано применение finetuning и переноса обучения."},{"metadata":{},"cell_type":"markdown","source":"Установим все необходимые библиотеки:"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#вот тут неплохо описано зачем эта команда https://andreyv.ru/nvidia-smi-poleznye-komandy.html\n!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport zipfile\nimport csv\nimport sys\nimport os\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.applications.efficientnet import EfficientNetB6\nfrom tensorflow.keras.applications.efficientnet import EfficientNetB4\nfrom tensorflow.keras.layers import *\n\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import RemoteMonitor\nfrom tensorflow.keras.callbacks import LambdaCallback\nfrom tensorflow.keras.callbacks import ProgbarLogger\n\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nimport PIL #Python Imaging Library\nfrom PIL import ImageOps, ImageFilter\n#увеличим дефолтный размер графиков\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n#графики в svg выглядят более четкими\n%config InlineBackend.figure_format = 'svg' \n%matplotlib inline\n\nprint(os.listdir(\"../input\"))\nprint('Python       :', sys.version.split('\\n')[0])\nprint('Numpy        :', np.__version__)\nprint('Tensorflow   :', tf.__version__)\nprint('Keras        :', tf.keras.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Работаем с Tensorflow v2**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Основные настройки"},{"metadata":{"trusted":true},"cell_type":"code","source":"# В setup выносим основные настройки: так удобнее их перебирать в дальнейшем.\n\nEPOCHS               = 5 # эпох на обучение\nBATCH_SIZE           = 16 #32 #64 # уменьшаем batch если сеть большая, иначе не влезет в память на GPU\nLR                   = 1e-4\nVAL_SPLIT            = 0.15 # сколько данных выделяем на тест = 15%\n\nCLASS_NUM            = 10  # количество классов в нашей задаче\nIMG_SIZE             = 128 # какого размера подаем изображения в сеть\n#IMG_SIZE =           (384, 512)\nIMG_CHANNELS         = 3   # у RGB 3 канала\ninput_shape          = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n#input_shape          = (IMG_SIZE[0], IMG_SIZE[1], IMG_CHANNELS)\n\nDATA_PATH = '../input/'\nPATH = \"../working/car/\" # рабочая директория","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nБыли опробованы следующие варианты параметров:\n*     EPOCHS = 5 и 10, чем больше эпох, тем выше скор, но падает скорость обучения\n*     BATCH_SIZE = 16 и 32 и 64, принудительно приходилось уменьшать, если сеть большая, иначе не влезет в память на GPU\n* IMG_SIZE = (384, 512) - самый лучший результат, пробовала варианты 128/224/384/512, когда длина и ширина изображения совпадают, для теста использовала 128 (так быстрее)\n\n\n    \n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Устаналиваем конкретное значение random seed для воспроизводимости\nos.makedirs(PATH,exist_ok=False)\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)  \nPYTHONHASHSEED = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA / Анализ данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(DATA_PATH+\"train.csv\")\nsample_submission = pd.read_csv(DATA_PATH+\"sample-submission.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.Category.value_counts()\n# распределение классов достаточно равномерное - это хорошо","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Распаковываем картинки')\n# Will unzip the files so that you can see them..\nfor data_zip in ['train.zip', 'test.zip']:\n    with zipfile.ZipFile(\"../input/\"+data_zip,\"r\") as z:\n        z.extractall(PATH)\n        \nprint(os.listdir(PATH))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Пример картинок (random sample)')\nplt.figure(figsize=(12,8))\n\nrandom_image = train_df.sample(n=9)\nrandom_image_paths = random_image['Id'].values\nrandom_image_cat = random_image['Category'].values\n\nfor index, path in enumerate(random_image_paths):\n    #To load an image from a file, use the open() function in the Image module:\n    im = PIL.Image.open(PATH+f'train/{random_image_cat[index]}/{path}')\n    plt.subplot(3,3, index+1)\n    plt.imshow(im)\n    plt.title('Class: '+str(random_image_cat[index]))\n    plt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмотрим на примеры картинок и их размеры чтоб понимать как их лучше обработать и сжимать."},{"metadata":{"trusted":true},"cell_type":"code","source":"image = PIL.Image.open(PATH+'/train/0/100380.jpg')\n#Once you have an instance of the Image class, you can use the methods defined by this class to process \n#and manipulate the image. For example, let’s display the image we just loaded:\nimgplot = plt.imshow(image)\nplt.show()\nimage.size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Подготовка данных"},{"metadata":{},"cell_type":"markdown","source":"### Аугментация данных"},{"metadata":{},"cell_type":"markdown","source":"**исходный вариант** - лучший результат для изображения 384*512"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Официальная документация: https://keras.io/preprocessing/image/\n#ImageDataGenerator Generate batches of tensor image data with real-time data augmentation.\n\n#The data will be looped over (in batches).\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    rotation_range = 5,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    validation_split=VAL_SPLIT, # set validation split\n    horizontal_flip=False)\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255) # вариант для сетей Xception и InceptionResNetV2\n#test_datagen = ImageDataGenerator() # без нормализации для сети EfficientNet\n#Рекомендация Подключите более продвинутые библиотеки аугментации изображений (например: albumentations или imgaug, для них есть специальные \"обертки\" под Keras, например: https://github.com/mjkvaak/ImageDataAugmentor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**вариант 2 с подобранными значениями** - лучший результат для изображения 512*512"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    rotation_range = 5, #Int. Degree range for random rotations. было 5, увеличим до 10,скор хуже, вернула 5\n    width_shift_range=0.2, # доля от общей ширины было 0.1, увеличим до 0.2 скор лучше\n    height_shift_range=0.2, # доля от общей высоты было 0.1, увеличим д0 0.2 скор лучше\n    validation_split=VAL_SPLIT, # set validation split\n    horizontal_flip=True) #Boolean. Randomly flip inputs horizontally.было False,скор лучше\n#vertical_flip=False, # Boolean. Randomly flip inputs vertically.по вертикали смысла нет, зачем машины вверх ногами\n#fill_mode = \"nearest\" пробовала все варианты, по умолчанию лучше, чем constant\",\"reflect\" и \"wrap\n#\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Изображение, считанное с диска, имеет формат uint8 (то есть unsigned integer 8-bit). Пиксели принимают целочисленные значения от 0 до 255. Операция нормализации (см. строку кода выше) переводит пиксели в формат float (число с плавающей запятой), и пиксели теперь принимают значения от 0 до 1. Это и называется нормализацией. Так вот, для Xception и InceptionResNetV2 нормализация нужна, а для EfficientNet не нужна."},{"metadata":{},"cell_type":"markdown","source":"Однако некоторые из этих архитектур имеют особенности. Например используя EfficientNetB0 ... EfficientNetB7 нужно пропускать шаг нормализации изображения, то есть на вход нейросети подавать изображения в формате uint8 от 0 до 255"},{"metadata":{},"cell_type":"markdown","source":"ImageDataGenerator хранит информацию о том, какие преобразования выполнять с изображениями, но не хранит информацию о том, откуда их брать.\nПри вызове метода flow_from_directory указывается, откуда брать изображения, каков размер батча и другие параметры. В итоге train_generator хранит информацию о том, откуда брать изображения и какие преобразования выполнять. Это не массив и даже не итератор, это просто набор информации. Каким же образом получать из него изображения?\nОбъект train_generator можно перебрать например циклом for:\nfor element in train_generator:\n    pass\nЕсли рассматривать устройство языка Python, то train_generator является объектом типа iterable, то есть имеет метод __iter__() . Вызвав этот метод, мы получим итератор. Итератор хранит позицию текущего элемента. Рекомендую почитать в интернете о том, как работают в питоне итераторы.\nИными словами, создавая train_generator мы указываем, что некоторые данные хранятся на диске, и указываем, как с ними работать. А именно мы говорим, что каждый раз получая из train_generator изображения мы применяем к ним аугментации. Например:\ndef get_all_data():\n    return list(train_generator)\nТаким образом мы загрузим все данные (изображения и метки) в массив. При каждом вызове метода get_all_data() мы будем получать массив изображений одной и той же длины, но с разными аугментациями. \nВыполнив код:\nelement = train_generator.__iter__().__next__()\nмы получим первый элемент из генератора. Это кортеж (tuple) из двух элементов: батча изображений и батча меток. Поэтому можно даже сделать так:\nimages_batch, labels_batch = train_generator.__iter__().__next__()\n\nЕсли суммировать все сказанное: при создании train_generator не создаются копии изображений. Они будут создаваться тогда, когда метод .fit() будет перебирать train_generator. На каждой эпохе он будет перебирать его заново и получать изображения с другими аугментациями."},{"metadata":{},"cell_type":"markdown","source":"### Аугментация данных с помощью albumentations"},{"metadata":{"trusted":true},"cell_type":"code","source":"#вариант реализации взят отсюда https://github.com/mjkvaak/ImageDataAugmentor/blob/master/examples/classification-with-flow_from_directory.ipynb\nAUGMENTATIONS = albumentations.Compose([\n    # flips\n    albumentations.HorizontalFlip(p=0.25),\n    #albumentations.VerticalFlip(p=0.25),закомментировала вертикальный поворот, изображение вверх ногами имеет сомнительную ценность\n    # color augmentations\n    albumentations.OneOf([\n        albumentations.HueSaturationValue(p=1.),\n        albumentations.RandomBrightnessContrast(p=1.),\n        albumentations.RGBShift(p=1.)\n    ], p=0.25),\n    # image quality\n    albumentations.OneOf([\n        albumentations.GaussNoise(p=1.),\n        albumentations.MultiplicativeNoise(p=1.),\n        albumentations.JpegCompression(p=1.),\n        albumentations.Downscale(scale_min=0.5,scale_max=0.99, p=1),\n    ], p=0.5),\n    # other\n    albumentations.OneOf([\n        albumentations.ToGray(p=1.0),\n        albumentations.RandomResizedCrop(height=IMG_SIZE, \n                                         width=IMG_SIZE),\n        \n    ], p=0.25),\n])\n\ntrain_datagen = ImageDataAugmentor(\n        rescale=1./255,\n        augment = AUGMENTATIONS,\n        validation_split=VAL_SPLIT,\n        )\n        \ntest_datagen = ImageDataAugmentor(rescale=1./255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#вариант реализации взят отсюда https://github.com/mjkvaak/ImageDataAugmentor\nAUGMENTATIONS = albumentations.Compose([\n    albumentations.Transpose(p=0.5),\n    albumentations.Flip(p=0.5),\n    albumentations.OneOf([\n        albumentations.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n        albumentations.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1)\n    ],p=1),\n    albumentations.GaussianBlur(p=0.05),\n    albumentations.HueSaturationValue(p=0.5),\n    albumentations.RGBShift(p=0.5),\n])\n\ntrain_datagen = ImageDataAugmentor(\n        rescale=1./255,\n        augment = AUGMENTATIONS,\n        validation_split=VAL_SPLIT,\n        )\n        \ntest_datagen = ImageDataAugmentor(rescale=1./255)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Генерация данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Завернем наши данные в генератор:\n#flow_from_directory - Takes the path to a directory & generates batches of augmented data.\ntrain_generator = train_datagen.flow_from_directory(\n    PATH+'train/',      # директория где расположены папки с картинками \n    target_size=(IMG_SIZE, IMG_SIZE),\n    #target_size=(IMG_SIZE[0], IMG_SIZE[1]),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True, seed=RANDOM_SEED,\n    subset='training') # set as training data\n\ntest_generator = train_datagen.flow_from_directory(\n    PATH+'train/',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    #target_size=(IMG_SIZE[0], IMG_SIZE[1]),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True, seed=RANDOM_SEED,\n    subset='validation') # set as validation data\n\ntest_sub_generator = test_datagen.flow_from_dataframe( \n    dataframe=sample_submission,\n    directory=PATH+'test_upload/',\n    x_col=\"Id\",\n    y_col=None,\n    shuffle=False,\n    class_mode=None,\n    seed=RANDOM_SEED,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    #target_size=(IMG_SIZE[0], IMG_SIZE[1]),\n    batch_size=BATCH_SIZE,)\n\n# Обратите внимание, что для сабмита мы используем другой источник test_datagen.flow_from_dataframe.\n#Потому что нам нужно дать предсказание с привязкой к конкретному Id картинки из dataframe.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# посмотрим на результаты аугментации\ntrain_generator.show_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Построение модели"},{"metadata":{},"cell_type":"markdown","source":"### Загружаем предобученные сети:"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = Xception(\n    weights='imagenet', # Подгружаем веса imagenet\n    include_top=False, # Выходной слой (голову) будем менять т.к. у нас други классы\n    input_shape = input_shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = EfficientNetB4(\n    include_top=False,  # Выходной слой (голову) будем менять т.к. у нас други классы\n    weights=\"imagenet\", # Подгружаем веса imagenet\n    input_shape = input_shape\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"при прочих равных условиях (кол-во эпох, размер изображения, вариант аугментации и т.д.) результат лучше, чем Xception на изображениях размером 128*128, на изображениях большего размера возникает ошибка ResourceExhaustedError"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = EfficientNetB6(\n    weights='imagenet', # Подгружаем веса imagenet\n    include_top=False,  # Выходной слой (голову) будем менять т.к. у нас други классы\n    input_shape=input_shape\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"при прочих равных условиях (кол-во эпох, размер изображения, вариант аугментации и т.д.) результат лучше, чем Xception, но хуже, чем у EfficientNetB4"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape = input_shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"при прочих равных условиях (кол-во эпох, размер изображения, вариант аугментации и т.д.) результат с базовой моделью InceptionResNetV2 хуже, даже чем Xception"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"base_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Устанавливаем новую \"голову\" (head)\n\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n# let's add a fully-connected layer\n#вот тут надо менять в зависимости от размера изображения \nx = Dense(128, activation='relu')(x)\n#x = Dense(IMG_SIZE[0],activation='relu')(x)\n#добавила Batch Normalization\nx =BatchNormalization()(x)\nx = Dropout(0.25)(x)\n# and a logistic layer -- let's say we have 10 classes\npredictions = Dense(CLASS_NUM, activation='softmax')(x)\n\n# this is the model we will train\nmodel = Model(inputs=base_model.input, outputs=predictions)\n#model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Добавление BatchNormalization немного улушило скор"},{"metadata":{},"cell_type":"markdown","source":"это фукнциональный вариант сборки модели, попробуем еще последовательный вариант. Функциональные модели гибче, можно не только связывать слой с предыдущим и последующим. Вообще рекомендую использовать функциональный стиль только тогда, когда по-другому невозможно, потому что функциональный стиль хуже читаем и повышает шанс ошибиться."},{"metadata":{},"cell_type":"markdown","source":"вариант последовательной модели (sequential) - с таким вариантом получен самый высокий скор "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential()\n\nmodel.add(base_model)\nmodel.add(GlobalAveragePooling2D(),)\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\n\n#операция с Sequential-моделью. \npredictions = model.add(Dense(CLASS_NUM, activation='softmax'))\n\n#Если у нас Sequantial-модель, то строка ниже не нужна\n#model = Model(inputs=base_model.input, outputs=predictions)\n\n#model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"этот вариант последовательной модели взят из одного из заданий, для проверки"},{"metadata":{"trusted":true},"cell_type":"code","source":"    #из DL3\nmodel = tf.keras.Sequential()\nmodel.add(base_model)\n#Conv2D - это сверточный слой\nmodel.add(Conv2D(16, kernel_size=3, strides=1, padding='same', input_shape=input_shape))\nmodel.add(Conv2D(32, kernel_size=3, strides=1, padding='same'))\nmodel.add(MaxPool2D())\n#для регуляризации сети слой  Dropout\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(32, kernel_size=3, strides=1, padding='same'))\nmodel.add(Conv2D(64, kernel_size=3, strides=1, padding='same'))\nmodel.add(MaxPool2D())\nmodel.add(Dropout(0.25))\n    #уплощение - вытягивание в вектор\nmodel.add(Flatten())\n    # чтобы получить больше параметров, добавим ещё один плотный слой перед крайним\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\n    #полносвязный слой - плотный слой у которого 10 выходов (у нас 10 классов) и функция активации softmax\npredictions = model.add(Dense(CLASS_NUM, activation='softmax'))\n    \n#model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"На последнем слое нельзя делать активацию relu, потому что все выходные значение меньше нуля обрезаются, то есть теряется информация на выходе. В промежуточных слоях relu используется для создания нелинейности, но в выходном слое нет смысла его использовать. Можно не указывать activation, либо указать activation='linear', что то же самое или softmax"},{"metadata":{},"cell_type":"markdown","source":"функция потерь \"categorical_crossentropy\", что то же самое, что tf.keras.losses.CategoricalCrossentropy(), но эта функция потерь подразумевает, что на выходном слое у вас активация softmax. Если не использовать softmax, то нужно указать параметр: tf.keras.losses.CategoricalCrossentropy(from_logits = True)"},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# сколько слоев\nprint(len(model.layers))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Количество параметров обучения\nlen(model.trainable_variables)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Статус слоев - будем обучать или нет\nfor layer in model.layers:\n    print(layer, layer.trainable)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Обучение модели"},{"metadata":{},"cell_type":"markdown","source":"**Step 1 - обучение \"головы\"**"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR=0.001\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Добавим ModelCheckpoint чтоб сохранять прогресс обучения модели и можно было потом подгрузить и дообучить модель."},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('best_model.hdf5' , monitor = ['val_accuracy'] , verbose = 1  , mode = 'max')\n\n#Enable visualizations for TensorBoard.\ntensboard= TensorBoard(\n    log_dir=\"logs\",\n    histogram_freq=0,\n    write_graph=True,\n    write_images=False,\n    update_freq=\"epoch\",\n    profile_batch=2,\n    embeddings_freq=0,\n    embeddings_metadata=None)\n\n#Stop training when a monitored metric has stopped improving. - вот это оч круто! останавливает обучение, когда\n#метрика перестает улучшаться, то есть сильно экономит время!\nearlystop = EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0,\n    patience=0,\n    verbose=0,\n    mode=\"auto\",\n    baseline=None,\n    restore_best_weights=False,\n)\n#Learning rate scheduler\n#scheduler = LearningRateScheduler(schedule, verbose=0)\n#https://keras.io/api/callbacks/learning_rate_scheduler/ надо функцию писать для шедулера\n\n#Reduce learning rate when a metric has stopped improving.\nreduceplateau = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.1,\n    patience=10,\n    verbose=0,\n    mode=\"auto\",\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0)\n\n#Callback used to stream events to a server.\n#monitor = RemoteMonitor(\n    #root=\"http://localhost:9000\",\n    #path=\"/publish/epoch/end/\",\n    #field=\"data\",\n    #headers=None,\n    #send_as_json=False)\n    \n#Callback for creating simple, custom callbacks on-the-fly.\n#lambda = LambdaCallback(\n    #on_epoch_begin=None,\n    #on_epoch_end=None,\n    #on_batch_begin=None,\n    #on_batch_end=None,\n    #on_train_begin=None,\n    #on_train_end=None)\n\n#Callback that prints metrics to stdout.\n#logger = ProgbarLogger(count_mode=\"samples\", stateful_metrics=None)\n\n\ncallbacks_list = [checkpoint, tensboard, earlystop, reduceplateau]\n\n\n# Рекомендация 2. Используйте разные техники управления Learning Rate\n# https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6 (eng)\n# http://teleported.in/posts/cyclic-learning-rate/ (eng)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Обучаем:"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(\n        train_generator,\n        steps_per_epoch = len(train_generator),\n        validation_data = test_generator, \n        validation_steps = len(test_generator),\n        epochs = EPOCHS,\n        callbacks = callbacks_list\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# сохраним итоговую сеть и подгрузим лучшую итерацию в обучении (best_model)\n#Saves the model to Tensorflow SavedModel or a single HDF5 file.\nmodel.save('../working/model_last.hdf5')\n#Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\nmodel.load_weights('best_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#evaluate_generator - это один из методов, как fit/predict\nscores = model.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  Посмотрим графики обучения:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n \nepochs = range(len(acc))\n \nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n \nplt.figure()\n \nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 2 - FineTuning - обучение половины весов**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Посмотрим на количество слоев в базовой модели\nprint(\"Number of layers in the base model: \", len(base_model.layers))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Разморозим базовую модель\nbase_model.trainable = True\n\n# Установим количество слоев, которые будем переобучать\nfine_tune_at = len(base_model.layers)//2\n\n# Заморозим первую половину слоев\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Количество параметров\nlen(base_model.trainable_variables)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Статус слоев - будем обучать или нет\nfor layer in model.layers:\n    print(layer, layer.trainable)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR=0.0001\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks_list = [checkpoint, tensboard, earlystop, reduceplateau]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Обучаем\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n    validation_data = test_generator, \n    validation_steps = test_generator.samples//test_generator.batch_size,\n    epochs = EPOCHS,\n    callbacks = callbacks_list\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('../working/model_step2.hdf5')\nmodel.load_weights('best_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate_generator - это один из методов, как fit/predict\nscores = model.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 3 - FineTuning - разморозка всей сети и дообучение**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Разморозим базовую модель\nbase_model.trainable = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR=0.00001\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks_list = [checkpoint, tensboard, earlystop, reduceplateau]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Обучаем\nhistory = model.fit_generator(\n        train_generator,\n        steps_per_epoch = train_generator.samples//train_generator.batch_size,\n        validation_data = test_generator, \n        validation_steps = test_generator.samples//test_generator.batch_size,\n        epochs = EPOCHS,\n        callbacks = callbacks_list\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('../working/model_step3.hdf5')\nmodel.load_weights('best_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate_generator - это один из методов, как fit/predict\nscores = model.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 4 - увеличение размера изображения**"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS               = 10\nBATCH_SIZE           = 4 # уменьшаем batch если сеть большая, иначе не влезет в память на GPU\nLR                   = 1e-4\n\nIMG_SIZE             = 512\nIMG_CHANNELS         = 3\ninput_shape          = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#лучший вариант параметров для изображения 512*512\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    rotation_range = 5, #Int. Degree range for random rotations. было 5, увеличим до 10,скор хуже, вернула 5\n    width_shift_range=0.2, # доля от общей ширины было 0.1, увеличим до 0.2 скор лучше\n    height_shift_range=0.2, # доля от общей высоты было 0.1, увеличим д0 0.2 скор лучше\n    validation_split=VAL_SPLIT, # set validation split\n    horizontal_flip=True) #Boolean. Randomly flip inputs horizontally.было False,скор лучше\n#vertical_flip=False, # Boolean. Randomly flip inputs vertically.по вертикали смысла нет, зачем машины вверх ногами\n#fill_mode = \"nearest\" пробовала все варианты, по умолчанию лучше, чем constant\",\"reflect\" и \"wrap\n#\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Завернем наши данные в генератор:\n#flow_from_directory - Takes the path to a directory & generates batches of augmented data.\ntrain_generator = train_datagen.flow_from_directory(\n    PATH+'train/',      # директория где расположены папки с картинками \n    target_size=(IMG_SIZE, IMG_SIZE),\n    #target_size=(IMG_SIZE[0], IMG_SIZE[1]),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True, seed=RANDOM_SEED,\n    subset='training') # set as training data\n\ntest_generator = train_datagen.flow_from_directory(\n    PATH+'train/',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    #target_size=(IMG_SIZE[0], IMG_SIZE[1]),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True, seed=RANDOM_SEED,\n    subset='validation') # set as validation data\n\ntest_sub_generator = test_datagen.flow_from_dataframe( \n    dataframe=sample_submission,\n    directory=PATH+'test_upload/',\n    x_col=\"Id\",\n    y_col=None,\n    shuffle=False,\n    class_mode=None,\n    seed=RANDOM_SEED,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    #target_size=(IMG_SIZE[0], IMG_SIZE[1]),\n    batch_size=BATCH_SIZE,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Заново создаем сеть с новым размером входных данных\nbase_model = Xception(\n    weights='imagenet', # Подгружаем веса imagenet\n    include_top=False, # Выходной слой (голову) будем менять т.к. у нас други классы\n    input_shape = input_shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential()\n\nmodel.add(base_model)\nmodel.add(GlobalAveragePooling2D(),)\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\n\n#операция с Sequential-моделью. \npredictions = model.add(Dense(CLASS_NUM, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks_list = [checkpoint, tensboard, earlystop, reduceplateau]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Обучаем\nhistory = model.fit_generator(\n        train_generator,\n        steps_per_epoch = train_generator.samples//train_generator.batch_size,\n        validation_data = test_generator, \n        validation_steps = test_generator.samples//test_generator.batch_size,\n        epochs = EPOCHS,\n        callbacks = callbacks_list\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('../working/model_step4.hdf5')\nmodel.load_weights('best_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = model.evaluate_generator(test_generator, verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Предсказание на тестовых данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"#это результат \"Завернем наши данные в генератор\"\ntest_sub_generator.samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub_generator.reset()\npredictions = model.predict_generator(test_sub_generator, steps=len(test_sub_generator), verbose=1) \npredictions = np.argmax(predictions, axis=-1) #multiple categories\nlabel_map = (train_generator.class_indices)\nlabel_map = dict((v,k) for k,v in label_map.items()) #flip k,v\npredictions = [label_map[k] for k in predictions]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filenames_with_dir=test_sub_generator.filenames\nsubmission = pd.DataFrame({'Id':filenames_with_dir, 'Category':predictions}, columns=['Id', 'Category'])\nsubmission['Id'] = submission['Id'].replace('test_upload/','')\nsubmission.to_csv('submission.csv', index=False)\nprint('Save submit')\n\n# Рекомендация: попробуйте добавить Test Time Augmentation (TTA)\n# https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Clean PATH\nimport shutil\nshutil.rmtree(PATH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"shutil.rmtree(path, ignore_errors=False, onerror=None) - Удаляет текущую директорию и все поддиректории; path должен указывать на директорию, а не на символическую ссылку."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}